{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10997b57-a7f1-4187-81f8-ab19a8fcc053",
   "metadata": {},
   "source": [
    "# Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a339d26a-80ed-47a8-9681-3d66f078ed51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2084] 2024-03-27 11:53:30,940 >> loading file tokenizer.model from cache at /home/users/h/huju/.cache/huggingface/hub/models--codellama--CodeLlama-7b-hf/snapshots/7f22f0a5f7991355a2c3867923359ec4ed0b58bf/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2084] 2024-03-27 11:53:30,941 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2084] 2024-03-27 11:53:30,942 >> loading file special_tokens_map.json from cache at /home/users/h/huju/.cache/huggingface/hub/models--codellama--CodeLlama-7b-hf/snapshots/7f22f0a5f7991355a2c3867923359ec4ed0b58bf/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2084] 2024-03-27 11:53:30,943 >> loading file tokenizer_config.json from cache at /home/users/h/huju/.cache/huggingface/hub/models--codellama--CodeLlama-7b-hf/snapshots/7f22f0a5f7991355a2c3867923359ec4ed0b58bf/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2084] 2024-03-27 11:53:30,944 >> loading file tokenizer.json from cache at /home/users/h/huju/.cache/huggingface/hub/models--codellama--CodeLlama-7b-hf/snapshots/7f22f0a5f7991355a2c3867923359ec4ed0b58bf/tokenizer.json\n",
      "[INFO|configuration_utils.py:726] 2024-03-27 11:53:31,233 >> loading configuration file config.json from cache at /home/users/h/huju/.cache/huggingface/hub/models--codellama--CodeLlama-7b-hf/snapshots/7f22f0a5f7991355a2c3867923359ec4ed0b58bf/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-03-27 11:53:31,235 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"codellama/CodeLlama-7b-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 16384,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.39.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32016\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/27/2024 11:53:31 - INFO - llmtuner.model.patcher - Using KV cache for faster generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3283] 2024-03-27 11:53:31,252 >> loading weights file model.safetensors from cache at /home/users/h/huju/.cache/huggingface/hub/models--codellama--CodeLlama-7b-hf/snapshots/7f22f0a5f7991355a2c3867923359ec4ed0b58bf/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1417] 2024-03-27 11:53:32,209 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:928] 2024-03-27 11:53:32,211 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|███████████████████████████| 2/2 [00:45<00:00, 22.65s/it]\n",
      "[INFO|modeling_utils.py:4024] 2024-03-27 11:54:17,886 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4032] 2024-03-27 11:54:17,886 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at codellama/CodeLlama-7b-hf.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:883] 2024-03-27 11:54:18,044 >> loading configuration file generation_config.json from cache at /home/users/h/huju/.cache/huggingface/hub/models--codellama--CodeLlama-7b-hf/snapshots/7f22f0a5f7991355a2c3867923359ec4ed0b58bf/generation_config.json\n",
      "[INFO|configuration_utils.py:928] 2024-03-27 11:54:18,045 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/27/2024 11:54:18 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
      "03/27/2024 11:54:19 - INFO - llmtuner.model.adapter - Merged 1 adapter(s).\n",
      "03/27/2024 11:54:19 - INFO - llmtuner.model.adapter - Loaded adapter(s): saves/Custom/lora/train_2024-03-26-16-50-46/checkpoint-1000\n",
      "03/27/2024 11:54:19 - INFO - llmtuner.model.loader - all params: 6738546688\n",
      "03/27/2024 11:54:19 - INFO - llmtuner.data.template - Add pad token: </s>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "User:  How many heads of the departments are older than 56 ? \"CREATE TABLE IF NOT EXISTS \\\"department\\\" (\\n\\\"Department_ID\\\" int,\\n\\\"Name\\\" text,\\n\\\"Creation\\\" text,\\n\\\"Ranking\\\" int,\\n\\\"Budget_in_Billions\\\" real,\\n\\\"Num_Employees\\\" real,\\nPRIMARY KEY (\\\"Department_ID\\\")\\n);\",             \"CREATE TABLE IF NOT EXISTS \\\"head\\\" (\\n\\\"head_ID\\\" int,\\n\\\"name\\\" text,\\n\\\"born_state\\\" text,\\n\\\"age\\\" real,\\nPRIMARY KEY (\\\"head_ID\\\")\\n);\",             \"CREATE TABLE IF NOT EXISTS \\\"management\\\" (\\n\\\"department_ID\\\" int,\\n\\\"head_ID\\\" int,\\n\\\"temporary_acting\\\" text,\\nPRIMARY KEY (\\\"Department_ID\\\",\\\"head_ID\\\"),\\nFOREIGN KEY (\\\"Department_ID\\\") REFERENCES `department`(\\\"Department_ID\\\"),\\nFOREIGN KEY (\\\"head_ID\\\") REFERENCES `head`(\\\"head_ID\\\")\\n);\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: SELECT COUNT(*), \\\"Name\\\" FROM \\\"department\\\" WHERE \\\"Ranking\\\" > 56 AND \\\"Num_Employees\\\" > 560000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "User:  How many heads of the departments are older than 56 ? CREATE TABLE IF NOT EXISTS \\\"head\\\" (\\n\\\"head_ID\\\" int,\\n\\\"name\\\" text,\\n\\\"born_state\\\" text,\\n\\\"age\\\" real,\\nPRIMARY KEY (\\\"head_ID\\\")\\n);\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: SELECT COUNT(*), \\\"name\\\" FROM \\\"head\\\" WHERE \\\"born_state\\\" > 56 AND \\\"age\\\" > 56 GROUP BY \\\"name\\\"\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "User:  How many heads of the departments are older than 56 ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: SELECT COUNT(*), \\\"name\\\" FROM \\\"departments\\\" WHERE \\\"born_state\\\" > 56 AND \\\"age\\\" > 56 GROUP BY \\\"name\\\"\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "User:  clear\n",
      "\n",
      "User:  How many heads of the departments are older than 56 ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: SELECT COUNT(Head) FROM Departments WHERE Age > 56\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "User:  Question: How many heads of the departments are older than 56 ?  Context: CREATE TABLE IF NOT EXISTS \"head\" (\"head_ID\" int,\"name\" text,\"born_state\" text,\"age\" real,PRIMARY KEY (\"head_ID\"));\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: SELECT COUNT(*) FROM \"head\" WHERE \"born_state\" = \"United States\" AND \"age\" > 56;\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m messages \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 10\u001b[0m   query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mUser: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m query\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "from llmtuner import ChatModel\n",
    "chat_model = ChatModel(dict(\n",
    "  model_name_or_path=\"codellama/CodeLlama-7b-hf\",\n",
    "  adapter_name_or_path=\"saves/Custom/lora/train_2024-03-26-16-50-46/checkpoint-1000\",\n",
    "  finetuning_type=\"lora\",\n",
    "  template=\"default\",\n",
    "))\n",
    "messages = []\n",
    "while True:\n",
    "  query = input(\"\\nUser: \")\n",
    "  if query.strip() == \"exit\":\n",
    "    break\n",
    "  if query.strip() == \"clear\":\n",
    "    messages = []\n",
    "    continue\n",
    "\n",
    "  messages.append({\"role\": \"user\", \"content\": query})\n",
    "  print(\"Assistant: \", end=\"\", flush=True)\n",
    "  response = \"\"\n",
    "  for new_text in chat_model.stream_chat(messages):\n",
    "    print(new_text, end=\"\", flush=True)\n",
    "    response += new_text\n",
    "  print()\n",
    "  messages.append({\"role\": \"assistant\", \"content\": response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f251a873-4d01-4c6c-b47e-fdbf5fe4b9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
