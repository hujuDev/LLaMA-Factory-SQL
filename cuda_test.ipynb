{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d20d7f7-b816-4cff-8b63-d9fa2dcbca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA available: \", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0406a348-8764-4580-89e0-6c3c86775c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/users/h/huju/miniconda3/envs/BA-LLaMA-Factory-SQL/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which -a python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2fda9f6-8c74-45d4-b96d-f25f9acb2c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/users/h/huju/miniconda3/envs/BA-LLaMA-Factory-SQL/bin/pip\n"
     ]
    }
   ],
   "source": [
    "!which pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b245e94-6233-4158-ae42-a36b45193e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Using cached wandb-0.16.5-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /home/users/h/huju/miniconda3/envs/BA-LLaMA-Factory-SQL/lib/python3.11/site-packages (from wandb) (8.1.7)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Using cached GitPython-3.1.42-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/users/h/huju/miniconda3/envs/BA-LLaMA-Factory-SQL/lib/python3.11/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/users/h/huju/miniconda3/envs/BA-LLaMA-Factory-SQL/lib/python3.11/site-packages (from wandb) (5.9.8)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Using cached sentry_sdk-1.43.0-py2.py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: PyYAML in /home/users/h/huju/miniconda3/envs/BA-LLaMA-Factory-SQL/lib/python3.11/site-packages (from wandb) (6.0.1)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Using cached setproctitle-1.3.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: setuptools in /home/users/h/huju/miniconda3/envs/BA-LLaMA-Factory-SQL/lib/python3.11/site-packages (from wandb) (68.2.2)\n",
      "Collecting appdirs>=1.4.3 (from wandb)\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting protobuf!=4.21.0,<5,>=3.19.0 (from wandb)\n",
      "  Using cached protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/users/h/huju/miniconda3/envs/BA-LLaMA-Factory-SQL/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/users/h/huju/miniconda3/envs/BA-LLaMA-Factory-SQL/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/users/h/huju/miniconda3/envs/BA-LLaMA-Factory-SQL/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/users/h/huju/miniconda3/envs/BA-LLaMA-Factory-SQL/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/users/h/huju/miniconda3/envs/BA-LLaMA-Factory-SQL/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Using cached wandb-0.16.5-py3-none-any.whl (2.2 MB)\n",
      "Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Using cached GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
      "Using cached protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Using cached sentry_sdk-1.43.0-py2.py3-none-any.whl (264 kB)\n",
      "Using cached setproctitle-1.3.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
      "Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: appdirs, smmap, setproctitle, sentry-sdk, protobuf, docker-pycreds, gitdb, GitPython, wandb\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.26.0\n",
      "    Uninstalling protobuf-5.26.0:\n",
      "      Successfully uninstalled protobuf-5.26.0\n",
      "Successfully installed GitPython-3.1.42 appdirs-1.4.4 docker-pycreds-0.4.0 gitdb-4.0.11 protobuf-4.25.3 sentry-sdk-1.43.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.5\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7b52c07-a618-479a-88b8-da927c55c783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /home/users/h/huju/miniconda3/envs/BA-LLaMA-Factory-SQL:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "_libgcc_mutex             0.1                        main  \n",
      "_openmp_mutex             5.1                       1_gnu  \n",
      "accelerate                0.28.0                   pypi_0    pypi\n",
      "aiofiles                  23.2.1                   pypi_0    pypi\n",
      "aiohttp                   3.9.3                    pypi_0    pypi\n",
      "aiosignal                 1.3.1                    pypi_0    pypi\n",
      "altair                    5.2.0                    pypi_0    pypi\n",
      "annotated-types           0.6.0                    pypi_0    pypi\n",
      "anyio                     4.3.0                    pypi_0    pypi\n",
      "attrs                     23.2.0                   pypi_0    pypi\n",
      "bitsandbytes              0.43.0                   pypi_0    pypi\n",
      "bzip2                     1.0.8                h5eee18b_5  \n",
      "ca-certificates           2024.3.11            h06a4308_0  \n",
      "certifi                   2024.2.2                 pypi_0    pypi\n",
      "charset-normalizer        3.3.2                    pypi_0    pypi\n",
      "click                     8.1.7                    pypi_0    pypi\n",
      "contourpy                 1.2.0                    pypi_0    pypi\n",
      "cycler                    0.12.1                   pypi_0    pypi\n",
      "datasets                  2.18.0                   pypi_0    pypi\n",
      "dill                      0.3.8                    pypi_0    pypi\n",
      "docstring-parser          0.16                     pypi_0    pypi\n",
      "einops                    0.7.0                    pypi_0    pypi\n",
      "fastapi                   0.110.0                  pypi_0    pypi\n",
      "ffmpy                     0.3.2                    pypi_0    pypi\n",
      "filelock                  3.13.3                   pypi_0    pypi\n",
      "fire                      0.6.0                    pypi_0    pypi\n",
      "fonttools                 4.50.0                   pypi_0    pypi\n",
      "frozenlist                1.4.1                    pypi_0    pypi\n",
      "fsspec                    2024.2.0                 pypi_0    pypi\n",
      "galore-torch              1.0                      pypi_0    pypi\n",
      "gradio                    3.50.2                   pypi_0    pypi\n",
      "gradio-client             0.6.1                    pypi_0    pypi\n",
      "h11                       0.14.0                   pypi_0    pypi\n",
      "httpcore                  1.0.4                    pypi_0    pypi\n",
      "httpx                     0.27.0                   pypi_0    pypi\n",
      "huggingface-hub           0.22.1                   pypi_0    pypi\n",
      "idna                      3.6                      pypi_0    pypi\n",
      "importlib-resources       6.4.0                    pypi_0    pypi\n",
      "jinja2                    3.1.3                    pypi_0    pypi\n",
      "jsonschema                4.21.1                   pypi_0    pypi\n",
      "jsonschema-specifications 2023.12.1                pypi_0    pypi\n",
      "kiwisolver                1.4.5                    pypi_0    pypi\n",
      "ld_impl_linux-64          2.38                 h1181459_1  \n",
      "libffi                    3.4.4                h6a678d5_0  \n",
      "libgcc-ng                 11.2.0               h1234567_1  \n",
      "libgomp                   11.2.0               h1234567_1  \n",
      "libstdcxx-ng              11.2.0               h1234567_1  \n",
      "libuuid                   1.41.5               h5eee18b_0  \n",
      "llmtuner                  0.6.0                    pypi_0    pypi\n",
      "markdown-it-py            3.0.0                    pypi_0    pypi\n",
      "markupsafe                2.1.5                    pypi_0    pypi\n",
      "matplotlib                3.8.3                    pypi_0    pypi\n",
      "mdurl                     0.1.2                    pypi_0    pypi\n",
      "mpmath                    1.3.0                    pypi_0    pypi\n",
      "multidict                 6.0.5                    pypi_0    pypi\n",
      "multiprocess              0.70.16                  pypi_0    pypi\n",
      "ncurses                   6.4                  h6a678d5_0  \n",
      "networkx                  3.2.1                    pypi_0    pypi\n",
      "numpy                     1.26.4                   pypi_0    pypi\n",
      "nvidia-cublas-cu12        12.1.3.1                 pypi_0    pypi\n",
      "nvidia-cuda-cupti-cu12    12.1.105                 pypi_0    pypi\n",
      "nvidia-cuda-nvrtc-cu12    12.1.105                 pypi_0    pypi\n",
      "nvidia-cuda-runtime-cu12  12.1.105                 pypi_0    pypi\n",
      "nvidia-cudnn-cu12         8.9.2.26                 pypi_0    pypi\n",
      "nvidia-cufft-cu12         11.0.2.54                pypi_0    pypi\n",
      "nvidia-curand-cu12        10.3.2.106               pypi_0    pypi\n",
      "nvidia-cusolver-cu12      11.4.5.107               pypi_0    pypi\n",
      "nvidia-cusparse-cu12      12.1.0.106               pypi_0    pypi\n",
      "nvidia-nccl-cu12          2.19.3                   pypi_0    pypi\n",
      "nvidia-nvjitlink-cu12     12.4.99                  pypi_0    pypi\n",
      "nvidia-nvtx-cu12          12.1.105                 pypi_0    pypi\n",
      "openssl                   3.0.13               h7f8727e_0  \n",
      "orjson                    3.9.15                   pypi_0    pypi\n",
      "packaging                 24.0                     pypi_0    pypi\n",
      "pandas                    2.2.1                    pypi_0    pypi\n",
      "peft                      0.10.0                   pypi_0    pypi\n",
      "pillow                    10.2.0                   pypi_0    pypi\n",
      "pip                       23.3.1          py311h06a4308_0  \n",
      "protobuf                  5.26.0                   pypi_0    pypi\n",
      "psutil                    5.9.8                    pypi_0    pypi\n",
      "pyarrow                   15.0.2                   pypi_0    pypi\n",
      "pyarrow-hotfix            0.6                      pypi_0    pypi\n",
      "pydantic                  2.6.4                    pypi_0    pypi\n",
      "pydantic-core             2.16.3                   pypi_0    pypi\n",
      "pydub                     0.25.1                   pypi_0    pypi\n",
      "pygments                  2.17.2                   pypi_0    pypi\n",
      "pyparsing                 3.1.2                    pypi_0    pypi\n",
      "python                    3.11.8               h955ad1f_0  \n",
      "python-dateutil           2.9.0.post0              pypi_0    pypi\n",
      "python-multipart          0.0.9                    pypi_0    pypi\n",
      "pytz                      2024.1                   pypi_0    pypi\n",
      "pyyaml                    6.0.1                    pypi_0    pypi\n",
      "readline                  8.2                  h5eee18b_0  \n",
      "referencing               0.34.0                   pypi_0    pypi\n",
      "regex                     2023.12.25               pypi_0    pypi\n",
      "requests                  2.31.0                   pypi_0    pypi\n",
      "rich                      13.7.1                   pypi_0    pypi\n",
      "rpds-py                   0.18.0                   pypi_0    pypi\n",
      "safetensors               0.4.2                    pypi_0    pypi\n",
      "scipy                     1.12.0                   pypi_0    pypi\n",
      "semantic-version          2.10.0                   pypi_0    pypi\n",
      "sentencepiece             0.2.0                    pypi_0    pypi\n",
      "setuptools                68.2.2          py311h06a4308_0  \n",
      "shtab                     1.7.1                    pypi_0    pypi\n",
      "six                       1.16.0                   pypi_0    pypi\n",
      "sniffio                   1.3.1                    pypi_0    pypi\n",
      "sqlite                    3.41.2               h5eee18b_0  \n",
      "sse-starlette             2.0.0                    pypi_0    pypi\n",
      "starlette                 0.36.3                   pypi_0    pypi\n",
      "sympy                     1.12                     pypi_0    pypi\n",
      "termcolor                 2.4.0                    pypi_0    pypi\n",
      "tk                        8.6.12               h1ccaba5_0  \n",
      "tokenizers                0.15.2                   pypi_0    pypi\n",
      "toolz                     0.12.1                   pypi_0    pypi\n",
      "torch                     2.2.1                    pypi_0    pypi\n",
      "tqdm                      4.66.2                   pypi_0    pypi\n",
      "transformers              4.39.1                   pypi_0    pypi\n",
      "triton                    2.2.0                    pypi_0    pypi\n",
      "trl                       0.8.1                    pypi_0    pypi\n",
      "typing-extensions         4.10.0                   pypi_0    pypi\n",
      "tyro                      0.7.3                    pypi_0    pypi\n",
      "tzdata                    2024.1                   pypi_0    pypi\n",
      "urllib3                   2.2.1                    pypi_0    pypi\n",
      "uvicorn                   0.29.0                   pypi_0    pypi\n",
      "websockets                11.0.3                   pypi_0    pypi\n",
      "wheel                     0.41.2          py311h06a4308_0  \n",
      "xxhash                    3.4.1                    pypi_0    pypi\n",
      "xz                        5.4.6                h5eee18b_0  \n",
      "yarl                      1.9.4                    pypi_0    pypi\n",
      "zlib                      1.2.13               h5eee18b_0  \n"
     ]
    }
   ],
   "source": [
    "!conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dfb4d9-a0ec-424e-bdd1-d589d3c02437",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/h/huju/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/users/h/huju/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/26/2024 17:59:34 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2084] 2024-03-26 17:59:34,353 >> loading file tokenizer.model from cache at /home/users/h/huju/.cache/huggingface/hub/models--codellama--CodeLlama-7b-hf/snapshots/7f22f0a5f7991355a2c3867923359ec4ed0b58bf/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2084] 2024-03-26 17:59:34,354 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2084] 2024-03-26 17:59:34,355 >> loading file special_tokens_map.json from cache at /home/users/h/huju/.cache/huggingface/hub/models--codellama--CodeLlama-7b-hf/snapshots/7f22f0a5f7991355a2c3867923359ec4ed0b58bf/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2084] 2024-03-26 17:59:34,356 >> loading file tokenizer_config.json from cache at /home/users/h/huju/.cache/huggingface/hub/models--codellama--CodeLlama-7b-hf/snapshots/7f22f0a5f7991355a2c3867923359ec4ed0b58bf/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2084] 2024-03-26 17:59:34,357 >> loading file tokenizer.json from cache at /home/users/h/huju/.cache/huggingface/hub/models--codellama--CodeLlama-7b-hf/snapshots/7f22f0a5f7991355a2c3867923359ec4ed0b58bf/tokenizer.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/26/2024 17:59:34 - INFO - llmtuner.data.template - Add pad token: </s>\n",
      "03/26/2024 17:59:34 - INFO - llmtuner.data.loader - Loading dataset b-mc2/sql-create-context...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting format of dataset: 100%|███████████████████████████████████████████████████████████████| 500/500 [00:00<00:00, 3623.81 examples/s]\n",
      "Running tokenizer on dataset: 100%|████████████████████████████████████████████████████████████████| 500/500 [00:00<00:00, 980.29 examples/s]\n",
      "[INFO|configuration_utils.py:726] 2024-03-26 17:59:44,584 >> loading configuration file config.json from cache at /home/users/h/huju/.cache/huggingface/hub/models--codellama--CodeLlama-7b-hf/snapshots/7f22f0a5f7991355a2c3867923359ec4ed0b58bf/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-03-26 17:59:44,587 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"codellama/CodeLlama-7b-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 16384,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.39.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32016\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:\n",
      "[12968, 29901, 1128, 1784, 15883, 310, 278, 5840, 1860, 526, 9642, 1135, 29871, 29945, 29953, 1577, 13, 27045, 10911, 2343, 313, 482, 2672, 4330, 17070, 29897, 13, 7900, 22137, 29901, 29871, 5097, 21122, 22798, 3895, 2343, 5754, 5046, 1405, 29871, 29945, 29953, 2]\n",
      "inputs:\n",
      "Human: How many heads of the departments are older than 56 ?\n",
      "CREATE TABLE head (age INTEGER)\n",
      "Assistant:  SELECT COUNT(*) FROM head WHERE age > 56</s>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5097, 21122, 22798, 3895, 2343, 5754, 5046, 1405, 29871, 29945, 29953, 2]\n",
      "labels:\n",
      "SELECT COUNT(*) FROM head WHERE age > 56</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3283] 2024-03-26 17:59:46,687 >> loading weights file model.safetensors from cache at /home/users/h/huju/.cache/huggingface/hub/models--codellama--CodeLlama-7b-hf/snapshots/7f22f0a5f7991355a2c3867923359ec4ed0b58bf/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1417] 2024-03-26 17:59:47,707 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:928] 2024-03-26 17:59:47,711 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [00:31<00:00, 15.72s/it]\n",
      "[INFO|modeling_utils.py:4024] 2024-03-26 18:00:19,737 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4032] 2024-03-26 18:00:19,738 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at codellama/CodeLlama-7b-hf.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:883] 2024-03-26 18:00:19,906 >> loading configuration file generation_config.json from cache at /home/users/h/huju/.cache/huggingface/hub/models--codellama--CodeLlama-7b-hf/snapshots/7f22f0a5f7991355a2c3867923359ec4ed0b58bf/generation_config.json\n",
      "[INFO|configuration_utils.py:928] 2024-03-26 18:00:19,907 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/26/2024 18:00:20 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.\n",
      "03/26/2024 18:00:20 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
      "03/26/2024 18:00:20 - INFO - llmtuner.model.utils - Found linear modules: gate_proj,up_proj,v_proj,q_proj,o_proj,down_proj,k_proj\n",
      "03/26/2024 18:00:22 - INFO - llmtuner.model.loader - trainable params: 19988480 || all params: 6758535168 || trainable%: 0.2958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/h/huju/miniconda3/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:607] 2024-03-26 18:00:22,838 >> Using auto half precision backend\n",
      "[INFO|trainer.py:1969] 2024-03-26 18:00:23,133 >> ***** Running training *****\n",
      "[INFO|trainer.py:1970] 2024-03-26 18:00:23,134 >>   Num examples = 500\n",
      "[INFO|trainer.py:1971] 2024-03-26 18:00:23,135 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:1972] 2024-03-26 18:00:23,136 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:1975] 2024-03-26 18:00:23,136 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:1976] 2024-03-26 18:00:23,137 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:1977] 2024-03-26 18:00:23,138 >>   Total optimization steps = 155\n",
      "[INFO|trainer.py:1978] 2024-03-26 18:00:23,142 >>   Number of trainable parameters = 19,988,480\n",
      "[INFO|integration_utils.py:723] 2024-03-26 18:00:23,149 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhuju\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/beegfs/home/users/h/huju/dev/BA/repos/LLaMA-Factory-SQL/wandb/run-20240326_180023-1kduikwf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/huju/codellama-7b-sql-finetune/runs/1kduikwf/workspace' target=\"_blank\">vibrant-river-1</a></strong> to <a href='https://wandb.ai/huju/codellama-7b-sql-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/huju/codellama-7b-sql-finetune' target=\"_blank\">https://wandb.ai/huju/codellama-7b-sql-finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/huju/codellama-7b-sql-finetune/runs/1kduikwf/workspace' target=\"_blank\">https://wandb.ai/huju/codellama-7b-sql-finetune/runs/1kduikwf/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='155' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 18/155 00:26 < 03:43, 0.61 it/s, Epoch 0.54/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.681000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from llmtuner import run_exp\n",
    "import wandb, os\n",
    "\n",
    "try:\n",
    "  assert torch.cuda.is_available() is True\n",
    "except AssertionError:\n",
    "  print(\"Please set up a GPU before using LLaMA Factory: https://medium.com/mlearning-ai/training-yolov4-on-google-colab-316f8fff99c6\")\n",
    "\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "wandb_project = \"codellama-7b-sql-finetune\"\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project\n",
    "\n",
    "run_exp(dict(\n",
    "  stage=\"sft\",\n",
    "  do_train=True,\n",
    "  model_name_or_path=\"codellama/CodeLlama-7b-hf\",\n",
    "  dataset=\"spider_sql\",\n",
    "  template=\"default\",\n",
    "  finetuning_type=\"lora\",\n",
    "  lora_target=\"all\",\n",
    "  output_dir=\"CodeLlama-7b-hf_run#2\",\n",
    "  per_device_train_batch_size=4,\n",
    "  gradient_accumulation_steps=4,\n",
    "  lr_scheduler_type=\"cosine\",\n",
    "  logging_steps=10,\n",
    "  save_steps=100,\n",
    "  learning_rate=5e-05,\n",
    "  num_train_epochs=5.0,\n",
    "  max_samples=500,\n",
    "  max_grad_norm=1.0,\n",
    "  fp16=True,\n",
    "  report_to=\"wandb\"\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
